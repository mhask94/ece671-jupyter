{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 20.  Eigenvalues and Eigenvectors\n",
    "Author: Mat Haskell - mhaskell9@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "Eigen values and eigen vectors are very cool and show up all over the place. Eigenvalues and eigenvectors exist generally for linear operators, but this notebook will just look at matrices. The basic idea is that certain vectors exist where, after the matrix operates on them, the direction of the vector has not changed. The magnitude can be changed, but not the direction. These special vectors are eigenvectors of the operator. This makes sense when thinking about a rotation matrix. If you try to rotate a vector purely in the x-direction about the x-axis, it does not rotate. This means that the axis of rotation must be an eigenvector of the rotation matrix since it cannot change the direction of the vector it is operating on.\n",
    "\n",
    "Another way to think about eigenvalues is that there are special cases where matrix multiplication is turned into scalar multiplication. This seems strange since the output of matrix multiplication is not really intuitive by just looking at the equation. Matrices are linear operators that can be used to accomplish various tasks. However, when multiplying a matrix by one of its eigenvectors, that vector will just be scaled by the value of the corresponding eigenvalue. The eigenvector can grow, shrink, and flip directions (i.e. eigenvalues can be negative). Eigenvalues can even be zero, in which case the eigenvector multiplied by the matrix will be scaled down to zero.\n",
    "\n",
    "The physical meaning of eigenvalues and eigenvectors is also cool. For special symmetric matrices, like inertia tensors, stress tensors, and strain tensors, the eigenvectors are the principal axes of the matrix. So if you were to rotate the original matrix axes to line up with the eigenvectors (or principal axes), all of the cross terms would go away leaving only numbers along the main diagonal. Also, the eigenvalues are the principal stresses/strains of a stress/strain tensor. Another physical meaning comes with differential and difference equations. Eigenvalues are the poles of the system and show stability based off if the eigenvalues are positive or negative. The last example I will include here is with Google. I don't know their algorithm, but I do know that Google's search engine optimization algorithm forms a giant matrix of info regarding links between websites and they use eigenvalues and eigenvectors to determine which websites are most closely related to your search. So eigenvalues are the magic that Google uses to make sure you don't have to look past the first few links to find what you are looking for! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\real}{\\mathbb{R}}$\n",
    "$\\newcommand{\\complex}{\\mathbb{C}}$\n",
    "$\\newcommand{\\script}[1]{\\mathcal{#1}}$\n",
    "$\\newcommand{\\chi}{\\script{X}}$\n",
    "## Explanation of the theory\n",
    "Eigenvalues only exist for square matrices.\n",
    "$$A\\in \\complex^{n\\times n}$$\n",
    "\n",
    "Let $x\\in\\complex^{n\\times1}$ be an eigenvector of A and $\\lambda\\in\\complex$ be the corresponding eigenvalue of A, then the idea that matrix multiplication is turned into scalar multiplication yields the equation:\n",
    "$$Ax=\\lambda x$$\n",
    "\n",
    "Here are the formal definitions for eigen pairs:\n",
    "\n",
    "__Def:__ $(\\lambda,x)$ is a __right eigen pair__ if $Ax=\\lambda x$.\n",
    "\n",
    "__Def:__ $(\\lambda,x)$ is a __left eigen pair__ if $x^HA^H=\\lambda x^H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time we deal with right eigen pairs. So here is how to actually solve for the eigenvalues. Before any computational work, we need to rearrange the equation $Ax=\\lambda x$ into a useful form.\n",
    "\n",
    "- Get everything onto 1 side of the equation (it doesn't matter which side):\n",
    "\n",
    "$$Ax-\\lambda x=0$$\n",
    "\n",
    "- Multiply x by identity (which doesn't change it):\n",
    "\n",
    "$$Ax-\\lambda Ix=0$$\n",
    "\n",
    "- Factor out the x:\n",
    "\n",
    "$$(A-\\lambda I)x=0$$\n",
    "\n",
    "__Note:__ some textbooks write it as $(\\lambda I-A)x=0$, but they are equivalent.\n",
    "\n",
    "We can see that $x\\in\\mathcal{N}(A-\\lambda I)$. Since there is a non-trivial null space we know that $(A-\\lambda I)$ is not full rank, which implies that $det(A-\\lambda I)=0$. We will use this fact to solve for the eigenvalues of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the eigenvalues of a matrix\n",
    "Here are the computational steps for finding the eigenvalues of A:\n",
    "\n",
    "1. Start with the equation we just derived:\n",
    "\n",
    "$$det(A-\\lambda I)=0$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\left|\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\begin{pmatrix}\n",
    "\\lambda & 0 & \\cdots & 0\\\\\n",
    "0 & \\lambda & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & \\lambda\n",
    "\\end{pmatrix}\n",
    "\\right|\n",
    "=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\begin{vmatrix}\n",
    "a_{11}-\\lambda & a_{12} & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22}-\\lambda & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn}-\\lambda\n",
    "\\end{vmatrix}\n",
    "=0\n",
    "$$\n",
    "\n",
    "2. Take the determinant. This will give the characteristic polynomial of A, $\\chi_A(\\lambda)$, and characteristic equation of A, $\\chi_A(\\lambda)=0$. Let $\\alpha_i$ be a scalar, then the charateristic polynomial can be written as:\n",
    "\n",
    "$$\\chi_A(\\lambda)\\triangleq\\lambda^n+\\alpha_{n-1}\\lambda^{n-1}+\\cdots+\\alpha_1\\lambda+\\alpha_0=0$$\n",
    "\n",
    "3. Find the roots of the characteristic polynomial, which are the eigenvalues. By the fundamental theorem of algebra, we know that the characteristic polynomial will have n roots. This means that there are going to be n eigenvalues, although some of them may be repeated. When eigenvalues are repeated, there are p distict eigenvalues, where p counts repeated eigenvalues only once.\n",
    "\n",
    "__Note:__ In practice, you will most likely never write an algorithm that finds eigenvalues. There are existing software tools to find eigenvalues (e.g. the eig() function in Matlab and Python). I would strongly recommend using those tools and not going through any of this process by hand! If you really wanted to find eigenvalues on paper, you could try polynomial division to get the roots. This can get tricky pretty fast as the number of roots increases (there is a lot of guessing and checking). Also, writing your own algorithm to find eigenvalues is more difficult than it might appear and you would need to do more research on your own to learn how. This is because numerical root finding algorithms typically only find real roots. Writing an algorithm to find complex roots is tricky and will not be discussed here. In fact, I believe that complex root finding tools actually use eigenvalue solvers to get the roots. Anyways, the Gershgorin Circle Theorem and Cauchy-Riemann Equations would be a good place to start your research.\n",
    "\n",
    "__Note:__ we can write $\\chi_A$ with all of the eigenvalues factored out (which would be the result of polynomial division):\n",
    "\n",
    "$$\\chi_A(\\lambda)=(\\lambda-\\lambda_{1})(\\lambda-\\lambda_{2})\\cdots(\\lambda-\\lambda_{n})=\\Pi_{i=0}^n(\\lambda-\\lambda_i)$$\n",
    "\n",
    "__Def:__ Algebraic multiplicity, $m_i$, of $\\lambda_i$ is the number of times it is repeated. So $\\chi_A$ can be written like this:\n",
    "$$\\chi_A(\\lambda)=(\\lambda-\\lambda_{1})^{m_1}(\\lambda-\\lambda_{2})^{m_2}\\cdots(\\lambda-\\lambda_{p})^{m_p},\\ \\ \\ \\ \\ p\\leq n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the eigenvectors of a matrix\n",
    "After finding the roots/eigenvalues, we can solve for the eigenvectors by plugging in each eigenvalue into the equation $(A-\\lambda_iI)x=0$. You are just solving for the null space of $(A-\\lambda I)$. \n",
    "\n",
    "__Note:__ any linear combination of vectors from $\\mathcal{N}(A-\\lambda I)$ are also eigenvectors. Because any scalar multiple of an eigenvector is also an eigenvector, most software libraries will return the eigenvectors with a norm of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Find the eigenvalues and eigenvectors of A by following the steps above.\n",
    "$$A=\\begin{pmatrix}4 & -2\\\\5 & -7\\end{pmatrix}$$\n",
    "\n",
    "1. First set $det(A-\\lambda I)=0$:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "4-\\lambda & -2\\\\\n",
    "5 & -7-\\lambda\n",
    "\\end{vmatrix}\n",
    "=0\n",
    "$$\n",
    "\n",
    "2. Take the determinant to find the characteristic equation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\chi_A(\\lambda)&=&(4-\\lambda)(-7-\\lambda)+10=0\\\\\n",
    "&=&\\lambda^2+3\\lambda-18=0\n",
    "\\end{eqnarray}\n",
    "\n",
    "3. Find the roots/eigenvalues.\n",
    "\n",
    "$$\\chi_A(\\lambda)=(\\lambda+6)(\\lambda-3)$$\n",
    "\n",
    "The order of assigning $\\lambda_1$ and $\\lambda_2$ doesn't really matter. I will use these:\n",
    "$$\\lambda_1=3$$ \n",
    "$$\\lambda_2=-6$$\n",
    "\n",
    "Neither of these are repeated, so the algebraic multiplicity of both are just 1.\n",
    "$$m_1=1$$ \n",
    "$$m_2=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with the example and find the eigenvectors of A.\n",
    "- For $\\lambda_1=3$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}\n",
    "4-\\lambda & -2\\\\\n",
    "5 & -7-\\lambda\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "4-3 & -2\\\\\n",
    "5 & -7-3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "1 & -2\\\\\n",
    "5 & -10\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Notice that both rows of A when multiplied by x give the same equation:\n",
    "\\begin{eqnarray}\n",
    "&x_1&-2x_2=0 \\\\\n",
    "\\Rightarrow &x_1&=2x_2\n",
    "\\end{eqnarray}\n",
    "Now we can solve for x:\n",
    "$$\n",
    "x=\n",
    "\\begin{pmatrix}2x_2 \\\\ x_2\\end{pmatrix}=\n",
    "\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}x_2\n",
    "$$\n",
    "An eigenvector associated with $\\lambda_1$, $\\mathbf{v_1}$, is any scalar multiple of $\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$.\n",
    "\n",
    "- For $\\lambda_2=-6$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}\n",
    "4-\\lambda & -2\\\\\n",
    "5 & -7-\\lambda\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "4+6 & -2\\\\\n",
    "5 & -7+6\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "10 & -2\\\\\n",
    "5 & -1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Notice that both rows of A when multiplied by x give the same equation:\n",
    "\\begin{eqnarray}\n",
    "&5x_1&-x_2=0 \\\\\n",
    "\\Rightarrow &x_2&=5x_1\n",
    "\\end{eqnarray}\n",
    "Now we can solve for x:\n",
    "$$\n",
    "x=\n",
    "\\begin{pmatrix}x_1 \\\\ 5x_1\\end{pmatrix}=\n",
    "\\begin{pmatrix}1 \\\\ 5\\end{pmatrix}x_1\n",
    "$$\n",
    "An eigenvector associated with $\\lambda_2$, $\\mathbf{v_2}$, is any scalar multiple of $\\begin{pmatrix}1 \\\\ 5\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue decomposition\n",
    "The eigenvalue decomposition of A is as such:\n",
    "$$A=S\\Lambda S^{-1}$$\n",
    "\n",
    "Where,\n",
    "$$\n",
    "\\Lambda=diag([\\lambda_1, \\lambda_2, \\cdots, \\lambda_n])\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & \\cdots & \\lambda_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S=\\begin{pmatrix}\n",
    "\\mathbf{v_1} & \\mathbf{v_2} & \\cdots & \\mathbf{v_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "__Note:__ it does not matter in which order the eigenvalues are placed to form $\\Lambda$. However, for $A=S\\Lambda S^{-1}$ to be true, the position of the column of $\\lambda_i$ must match the position of the column of $\\mathbf{v_i}$ (where $\\mathbf{v_i}$ is an eigenvector corresponding to $\\lambda_i$).\n",
    "\n",
    "__Def:__ Geometric multiplicity, $q_i$, of $\\lambda_i$ is the number of linearly independent eigenvectors that can be formed from $\\lambda_i$. Also, $1\\leq q_i\\leq m_i$.\n",
    "This means at least 1 eigenvector can be formed from every eigenvalue, but at most $m_i$. For example: if an eigenvalue is repeated 3 times, there will be at most 3 corresponding linearly independent eigenvectors but at least 1 eigenvector.\n",
    "\n",
    "The geometric multiplicity of an eigenvalue is a very important concept for eigenvalue decomposition. In order to perform the eigenvalue decomposition, as described above where $A=S\\Lambda S^{-1}$, the geomectric multiplicity must be equal to the algebraic multiplicity for each eigenvalue. i.e.\n",
    "\n",
    "$$m_i=q_i\\ \\ \\ \\forall\\lambda_i$$\n",
    "\n",
    "This means that however many times an eigenvalue is repeated, there must be that same number of linearly independent eigenvectors derived from that eigenvalue. Otherwise, there isn't enough eigenvectors to fill the columns of S.\n",
    "\n",
    "When $m_i\\neq q_i$, there is a process to find what are called generalized eigenvectors to fill up the missing columns of S. When this happens, $\\Lambda$ is replaced with $J$ and is not strictly diagonal. This is called the Jordan form and is a topic for another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun Facts \n",
    "- If an eigenvalue is complex, its complex conjugate will also be an eigenvalue. \n",
    "- If an eigenvector is complex, its complex conjugate will also be an eigenvector.\n",
    "- Symmetric matrices have real eigenvalues.\n",
    "- For a positive definite matrix $\\Leftrightarrow$ all eigenvalues will be positive ($\\lambda_i>0$).\n",
    "- For a positive semi-definite matrix $\\Leftrightarrow$ all eigenvalues will be non-negative ($\\lambda_i\\geq0$).\n",
    "- For a negative semi-definite matrix $\\Leftrightarrow$ all eigenvalues will be non-positive ($\\lambda_i\\leq0$).\n",
    "- For a negative definite matrix $\\Leftrightarrow$ all eigenvalues will be negative ($\\lambda_i<0$).\n",
    "- The trace of a matrix equals the sum of its eigenvalues ($trace(A)=\\sum\\lambda_i$).\n",
    "- The determinant of a matrix equals the product of its eigenvalues ($det(A)=\\Pi\\lambda_i$).\n",
    "- Eigenvectors of a matrix are all linearly independent.\n",
    "- The eigenvalues of a diagonal matrix are all of the elements along the diagonal.\n",
    "- For every 0 eigenvalue, the rank of a matrix goes down by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Numerical Examples\n",
    "\n",
    "### Setup structure for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.i = 1\n",
    "    def runTest(self,check,statement):\n",
    "        print('Test %d: ' % self.i)\n",
    "        self.i += 1\n",
    "        if check:\n",
    "            print(statement + ' \\n')\n",
    "        else:\n",
    "            print('error \\n')\n",
    "            \n",
    "def printEigen(vals,vecs):\n",
    "    print('Eigenvalues are {:.2f}, {:.2f}, and {:.2f}. \\n'.format(vals[0],vals[1],vals[2]))\n",
    "    print('The eigenvector corresponding to {:.2f} is:'.format(vals[0]))\n",
    "    print(vecs[:,0][:,None])\n",
    "    print('The eigenvector corresponding to {:.2f} is:'.format(vals[1]))\n",
    "    print(vecs[:,1][:,None])\n",
    "    print('The eigenvector corresponding to {:.2f} is:'.format(vals[2]))\n",
    "    print(vecs[:,2][:,None])\n",
    "    print('') # add a blank line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: \n",
      "v1 and l1 are a right eigen pair! \n",
      "\n",
      "Test 2: \n",
      "v2 and l2 are a right eigen pair! \n",
      "\n",
      "Test 3: \n",
      "A = S*L*S_inv! \n",
      "\n",
      "Test 4: \n",
      "It still works when the order is switched! \n",
      "\n",
      "Test 5: \n",
      "The trace equals sum of eigenvalues! \n",
      "\n",
      "Test 6: \n",
      "error \n",
      "\n",
      "Test 7: \n",
      "The determinant equals product of eigenvalues! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the matrix we used above\n",
    "A = np.array([[4,-2],\n",
    "              [5,-7]])\n",
    "# these are the eigenvalues we found\n",
    "l1 = 3\n",
    "l2 = -6\n",
    "# these are the eigenvectors we found\n",
    "v1 = np.array([[2],\n",
    "               [1]])\n",
    "v2 = np.array([[1],\n",
    "               [5]])\n",
    "\n",
    "# check that Ax-lx=0\n",
    "# note that numerical precision might limit this to being very close to true\n",
    "tests = Test()\n",
    "\n",
    "test1 = np.array_equal(A@v1, l1*v1)\n",
    "tests.runTest(test1,'v1 and l1 are a right eigen pair!')\n",
    "\n",
    "test2 = np.array_equal(A@v2, l2*v2)\n",
    "tests.runTest(test2,'v2 and l2 are a right eigen pair!')\n",
    "    \n",
    "# eigenvalue decomposition\n",
    "L = np.diag([l1, l2])\n",
    "S = np.concatenate((v1,v2), axis=1)\n",
    "S_inv = np.linalg.inv(S)\n",
    "\n",
    "# test that it works\n",
    "test3 = np.array_equal(A, S@L@S_inv)\n",
    "tests.runTest(test3,'A = S*L*S_inv!')\n",
    "\n",
    "# now let's test that the order doesn't matter\n",
    "L = np.diag([l2, l1])\n",
    "S = np.concatenate((v2,v1), axis=1)\n",
    "S_inv = np.linalg.inv(S)\n",
    "\n",
    "test4 = np.array_equal(A, S@L@S_inv)\n",
    "tests.runTest(test4,'It still works when the order is switched!')\n",
    "\n",
    "# test that trace equals sum of eigenvalues\n",
    "test5 = np.trace(A)==l1+l2\n",
    "tests.runTest(test5,'The trace equals sum of eigenvalues!')\n",
    "\n",
    "# test that determinant equals product of eigenvalues\n",
    "test6 = np.linalg.det(A)==l1*l2\n",
    "tests.runTest(test6,'The determinant equals product of eigenvalues!')\n",
    "# this runs into numerical precision errors, try again testing if error is small\n",
    "det_error = np.linalg.det(A)-l1*l2\n",
    "test7 = np.abs(det_error) < 1e-10\n",
    "tests.runTest(test7,'The determinant equals product of eigenvalues!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see how to get the eigenvalues through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues are 2.00+1.00j, 2.00-1.00j, and 2.00+0.00j. \n",
      "\n",
      "The eigenvector corresponding to 2.00+1.00j is:\n",
      "[[ 0.        +0.j        ]\n",
      " [ 0.81649658+0.j        ]\n",
      " [-0.40824829-0.40824829j]]\n",
      "The eigenvector corresponding to 2.00-1.00j is:\n",
      "[[ 0.        -0.j        ]\n",
      " [ 0.81649658-0.j        ]\n",
      " [-0.40824829+0.40824829j]]\n",
      "The eigenvector corresponding to 2.00+0.00j is:\n",
      "[[1.+0.j]\n",
      " [0.+0.j]\n",
      " [0.+0.j]]\n",
      "\n",
      "Test 1: \n",
      "A*x1 = l1*x1! \n",
      "\n",
      "Test 2: \n",
      "A*x2 = l2*x2! \n",
      "\n",
      "Test 3: \n",
      "A*x3 = l3*x3! \n",
      "\n",
      "Test 4: \n",
      "Eigenvalue decomposition worked! \n",
      "\n",
      "Test 5: \n",
      "Eigenvalues came in complex conjugate pairs! \n",
      "\n",
      "Test 6: \n",
      "Eigenvectors came in complex conjugate pairs! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2,0,0],\n",
    "              [0,1,-2],\n",
    "              [0,1,3]])\n",
    "\n",
    "vals,vecs = np.linalg.eig(A)\n",
    "\n",
    "printEigen(vals,vecs)\n",
    "\n",
    "# check if Ax-lx=0\n",
    "# due to numerical precision error, I will check if norm(Ax-lx) < 1e-10\n",
    "tests = Test()\n",
    "\n",
    "test1 = np.linalg.norm(A@vecs[:,0] - vals[0]*vecs[:,0]) < 1e-10\n",
    "tests.runTest(test1,'A*x1 = l1*x1!')\n",
    "\n",
    "test2 = np.linalg.norm(A@vecs[:,1] - vals[1]*vecs[:,1]) < 1e-10\n",
    "tests.runTest(test2,'A*x2 = l2*x2!')\n",
    "\n",
    "test3 = np.linalg.norm(A@vecs[:,2] - vals[2]*vecs[:,2]) < 1e-10\n",
    "tests.runTest(test3,'A*x3 = l3*x3!')\n",
    "\n",
    "\n",
    "# Eigenvalue decomposition\n",
    "S = vecs\n",
    "Lambda = np.diag([vals[0],vals[1],vals[2]])\n",
    "S_inv = np.linalg.inv(S)\n",
    "\n",
    "# test that A = S*Lambda*S_inv\n",
    "test4 = np.linalg.norm(A - S@Lambda@S_inv) < 1e-10\n",
    "tests.runTest(test4,'Eigenvalue decomposition worked!')\n",
    "\n",
    "# test if eigenvalues came in complex conjugate pairs\n",
    "test5 = vals[0].conj()==vals[1]\n",
    "tests.runTest(test5,'Eigenvalues came in complex conjugate pairs!')\n",
    "\n",
    "# test if eigenvectors came in complex conjugate pairs\n",
    "test6 = np.array_equal(vecs[:,0].conj(), vecs[:,1])\n",
    "tests.runTest(test6,'Eigenvectors came in complex conjugate pairs!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: \n",
      "The rank went down by 1 for each 0 eigenvalue! \n",
      "\n",
      "Test 2: \n",
      "Repeated eigenvalues did not produce linearly independent eigenvectors \n",
      "\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[1,3,4],\n",
    "              [1,3,4],\n",
    "              [1,3,4]])\n",
    "\n",
    "vals,vecs = np.linalg.eig(B)\n",
    "\n",
    "# test that rank goes down by 1 for each 0 eigenvalue\n",
    "num_0_vals = 0\n",
    "for x in vals:\n",
    "    if x==0:\n",
    "        num_0_vals += 1\n",
    "\n",
    "m,n = B.shape      \n",
    "max_rank = min(m,n)\n",
    "expected_rank = max_rank - num_0_vals\n",
    "\n",
    "tests = Test()\n",
    "\n",
    "test1 = expected_rank==np.linalg.matrix_rank(B)\n",
    "tests.runTest(test1,'The rank went down by 1 for each 0 eigenvalue!')\n",
    "\n",
    "# show that repeated eigenvalues don't always produce lin. ind. eigenvectors\n",
    "test2 = np.array_equal(vecs[:,0], -vecs[:,2]) and vals[0]==vals[2]\n",
    "tests.runTest(test2,'Repeated eigenvalues did not produce linearly independent eigenvectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Engineering Application\n",
    "### Find the axis and angle of rotation from a rotation matrix\n",
    "It turns out that the eigenvalues of a 3 dimensional rotation matrix will always be 1, $e^{j\\theta}$, and $e^{-j\\theta}$. Also, the eigenvector associated with $\\lambda=1$ is the axis of rotation. You could also find these values through other means, like quaternions or axis-angle parameterization. But it is cool that the eigenvalues and eigenvectors have real physical meaning for rotation matrices!\n",
    "\n",
    "__Note:__ $e^{j\\theta}=cos(\\theta)+j sin(\\theta)$, so you find the magnitude of theta with $acos(real(\\lambda))$. There are limitations of acos() and asin() that should be considered as well. I will use positve angles below 90 degrees to avoid issues here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues are 0.71+0.71j, 0.71-0.71j, and 1.00+0.00j. \n",
      "\n",
      "The eigenvector corresponding to 0.71+0.71j is:\n",
      "[[0.        +0.j        ]\n",
      " [0.70710678+0.j        ]\n",
      " [0.        -0.70710678j]]\n",
      "The eigenvector corresponding to 0.71-0.71j is:\n",
      "[[0.        -0.j        ]\n",
      " [0.70710678-0.j        ]\n",
      " [0.        +0.70710678j]]\n",
      "The eigenvector corresponding to 1.00+0.00j is:\n",
      "[[1.+0.j]\n",
      " [0.+0.j]\n",
      " [0.+0.j]]\n",
      "\n",
      "Test 1: \n",
      "The eigenvector associated with lambda=1 is the axis of rotation! \n",
      "\n",
      "Test 2: \n",
      "The eigenvalue gave the correct amount of rotation! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rotx(angle):\n",
    "    R = np.array([[1,0,0],\n",
    "                  [0,np.cos(angle),-np.sin(angle)],\n",
    "                  [0,np.sin(angle),np.cos(angle)]])\n",
    "    return R\n",
    "def roty(angle):\n",
    "    R = np.array([[np.cos(angle),0,np.sin(angle)],\n",
    "                  [0,1,0],\n",
    "                  [-np.sin(angle),0,np.cos(angle)]])\n",
    "    return R\n",
    "def rotz(angle):\n",
    "    R = np.array([[np.cos(angle),-np.sin(angle),0],\n",
    "                  [np.sin(angle),np.cos(angle),0],\n",
    "                  [0,0,1]])\n",
    "    return R\n",
    "\n",
    "tests = Test()\n",
    "\n",
    "# test simple rotation about x-axis\n",
    "theta_x = np.pi/4\n",
    "Rx = rotx(theta_x)\n",
    "x_axis = np.array([[1],[0],[0]])\n",
    "\n",
    "vals,vecs = np.linalg.eig(Rx)\n",
    "\n",
    "# check to see if axis of rotation from eigenvector is the x-axis\n",
    "printEigen(vals,vecs)\n",
    "index = np.where(vals==1)[0]\n",
    "axis_of_rotation = vecs[:,index[0]][:,None]\n",
    "\n",
    "test1 = np.array_equal(x_axis,axis_of_rotation)\n",
    "tests.runTest(test1,'The eigenvector associated with lambda=1 is the axis of rotation!')\n",
    "\n",
    "# check that the amount of rotation from eigenvalue is correct\n",
    "angle = np.arccos(np.real(vals[0]))\n",
    "\n",
    "test2 = angle==theta_x\n",
    "tests.runTest(test2,'The eigenvalue gave the correct amount of rotation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues are 1.00+0.00j, 0.34+0.94j, and 0.34-0.94j. \n",
      "\n",
      "The eigenvector corresponding to 1.00+0.00j is:\n",
      "[[-0.73983182+0.j]\n",
      " [-0.19683033+0.j]\n",
      " [-0.64335581+0.j]]\n",
      "The eigenvector corresponding to 0.34+0.94j is:\n",
      "[[-0.10502437+0.46399819j]\n",
      " [ 0.69327405+0.j        ]\n",
      " [-0.09132891-0.53357819j]]\n",
      "The eigenvector corresponding to 0.34-0.94j is:\n",
      "[[-0.10502437-0.46399819j]\n",
      " [ 0.69327405-0.j        ]\n",
      " [-0.09132891+0.53357819j]]\n",
      "\n",
      "Test 3: \n",
      "The eigenvector associated with lambda=1 is the axis of rotation! \n",
      "\n",
      "Test 4: \n",
      "The eigenvalue gave the correct amount of rotation! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now test a case where there is rotation about all 3 axes\n",
    "theta_y = np.pi/6\n",
    "theta_z = np.pi/5\n",
    "Ry = roty(theta_y)\n",
    "Rz = rotz(theta_z)\n",
    "R = Rx@Ry@Rz\n",
    "\n",
    "vals,vecs = np.linalg.eig(R)\n",
    "\n",
    "# get the axis-angle parameterization\n",
    "expected_angle = np.arccos(0.5*(np.trace(R)-1))\n",
    "expected_axis = np.array([[R[1,2]-R[2,1]],\n",
    "                         [R[2,0]-R[0,2]],\n",
    "                         [R[0,1]-R[1,0]]])/(2*np.sin(expected_angle))\n",
    "\n",
    "# check to see if axis of rotation from eigenvector is correct\n",
    "printEigen(vals,vecs)\n",
    "eigen_axis = vecs[:,0]\n",
    "\n",
    "test3 = np.array_equal(expected_axis, eigen_axis)\n",
    "tests.runTest(test1,'The eigenvector associated with lambda=1 is the axis of rotation!')\n",
    "\n",
    "# check that the amount of rotation from eigenvalue is correct\n",
    "eigen_angle = np.arccos(np.real(vals[2]))\n",
    "\n",
    "test4 = expected_angle==eigen_angle\n",
    "tests.runTest(test2,'The eigenvalue gave the correct amount of rotation!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
