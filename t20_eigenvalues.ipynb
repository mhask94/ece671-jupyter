{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 20.  Eigenvalues and Eigenvectors\n",
    "Author: Mat Haskell - mhaskell9@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "Eigen values and eigen vectors are very cool and show up all over the place. Eigenvalues exist generally for linear operators, but this notebook will just look at matrices. The basic idea is that matrix multiplication is turned into scalar multiplication. This seems strange since matrix multiplication is complex and you can't always envision the result by just looking at the equation. Matrices can be used to project onto a lower dimensional space and rotate a vector, which is how they are commonly used. However, when multiplying a matrix by one of its eigenvectors, that vector will just be scaled by the value of the corresponding eigenvalue. The eigenvector can grow, shrink, and flip directions (i.e. eigenvalues can be negative). Eigenvalues can even be zero, in which case the eigenvector multiplied by the matrix will be scaled down to zero as well.\n",
    "\n",
    "__PREFACE THIS__\n",
    "\n",
    "The physical meaning of eigenvalues and eigenvectors is also cool. The eigenvectors are the principal axes of the matrix. This means that the eigenvectors of a stress/strain tensor are the principal axes and the eigenvalues are the principal stresses/strains. Another example is with an inertia tensor. If there are cross terms (elements off the main diagonal in the inertia tensor) the eigenvectors are the axes of the object that get rid of the cross terms. This means you can find the x, y, and z axes where there is symmetry and no inertia in any other direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\real}{\\mathbb{R}}$\n",
    "$\\newcommand{\\complex}{\\mathbb{C}}$\n",
    "$\\newcommand{\\script}[1]{\\mathcal{#1}}$\n",
    "$\\newcommand{\\chi}{\\script{X}}$\n",
    "## Explanation of the theory\n",
    "Eigenvalues only exist for square matrices.\n",
    "$$A\\in \\complex^{n\\times n}$$\n",
    "\n",
    "Let $x\\in\\complex^{n\\times1}$ be an eigenvector of A and $\\lambda\\in\\complex$ be the corresponding eigenvalue of A, then converting matrix multiplication into scalar multiplication yields the equation:\n",
    "$$Ax=\\lambda x$$\n",
    "\n",
    "Here are the formal definitions for eigen pairs:\n",
    "\n",
    "__Def:__ $(\\lambda,x)$ is a __right eigen pair__ if $Ax=\\lambda x$.\n",
    "\n",
    "__Def:__ $(\\lambda,x)$ is a __left eigen pair__ if $x^HA^H=\\lambda x^H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time we deal with right eigen pairs. So here is how to actually solve for the eigenvalues. Before any computational work, we need to rearrange the equation $Ax=\\lambda x$ into a useful form.\n",
    "\n",
    "- Get everything onto 1 side of the equation (it doesn't matter which side):\n",
    "\n",
    "$$Ax-\\lambda x=0$$\n",
    "\n",
    "- Multiply x by identity (which doesn't change it):\n",
    "\n",
    "$$Ax-\\lambda Ix=0$$\n",
    "\n",
    "- Factor out the x:\n",
    "\n",
    "$$(A-\\lambda I)x=0$$\n",
    "\n",
    "__Note:__ some textbooks write it as $(\\lambda I-A)x=0$, but they are equivalent.\n",
    "\n",
    "We can see that $x\\in\\mathcal{N}(A-\\lambda I)$. Since there is a non-trivial null space we know that $(A-\\lambda I)$ is not full rank, which implies that $det(A-\\lambda I)=0$. We will use this fact to solve for the eigenvalues of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the eigenvalues of a matrix\n",
    "Here are the computational steps for finding the eigenvalues of A:\n",
    "\n",
    "1. Start with the equation we just derived:\n",
    "\n",
    "$$det(A-\\lambda I)=0$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\left|\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\begin{pmatrix}\n",
    "\\lambda & 0 & \\cdots & 0\\\\\n",
    "0 & \\lambda & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & \\lambda\n",
    "\\end{pmatrix}\n",
    "\\right|\n",
    "=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\left|\n",
    "\\begin{pmatrix}\n",
    "a_{11}-\\lambda & a_{12} & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22}-\\lambda & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn}-\\lambda\n",
    "\\end{pmatrix}\n",
    "\\right|\n",
    "=0\n",
    "$$\n",
    "\n",
    "2. Take the determinant. This will give the characteristic polynomial of A, $\\chi_A(\\lambda)$, and characteristic equation of A, $\\chi_A(\\lambda)=0$. Let $\\alpha_i$ be a scalar, then the charateristic polynomial can be written as:\n",
    "\n",
    "$$\\chi_A(\\lambda)\\triangleq\\lambda^n+\\alpha_{n-1}\\lambda^{n-1}+\\cdots+\\alpha_1\\lambda+\\alpha_0=0$$\n",
    "\n",
    "3. Find the roots of the characteristic polynomial, which are the eigenvalues. By the fundamental theorem of algebra, we know that the characteristic polynomial will have n roots. This means that there are going to be n eigenvalues, although some of them may be repeated. When eigenvalues are repeated, there are p distict eigenvalues, where p does not count a repeated eigenvalue more than once. There are software libraries that can solve the roots of a polynomial, or you can do it by hand with polynomial division. This involves some guessing and checking because you have to guess a root, do the polynomial division, and check if it worked (and repeat). Since software tools exist, I would suggest not finding the roots by hand. In fact, I wouldn't even find the eigenvalues or eigenvectors by hand since Matlab and Python already have functions for them. But hopefully you can see how it can be done from this notebook.\n",
    "\n",
    "__Note:__ we can write $\\chi_A$ with all of the eigenvalues factored out (which would be the result of polynomial division):\n",
    "\n",
    "$$\\chi_A(\\lambda)=(\\lambda-\\lambda_{1})(\\lambda-\\lambda_{2})\\cdots(\\lambda-\\lambda_{n})=\\Pi_{i=0}^n(\\lambda-\\lambda_i)$$\n",
    "\n",
    "__Def:__ Algebraic multiplicity, $m_i$, of $\\lambda_i$ is the number of times it is repeated. So $\\chi_A$ can be written like this:\n",
    "$$\\chi_A(\\lambda)=(\\lambda-\\lambda_{1})^{m_1}(\\lambda-\\lambda_{2})^{m_2}\\cdots(\\lambda-\\lambda_{p})^{m_p},\\ \\ \\ \\ \\ p\\leq n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the eigenvectors of a matrix\n",
    "After finding the roots/eigenvalues, we can solve for the eigenvectors by plugging in each eigenvalue into the equation $(A-\\lambda_iI)x=0$. You are just solving for the null space of $(A-\\lambda I)$. \n",
    "\n",
    "__Note:__ any linear combination of vectors from $\\mathcal{N}(A-\\lambda I)$ are also eigenvectors. Because any scalar multiple of an eigenvector is also an eigenvector, most software libraries will return the eigenvectors with a norm of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Find the eigenvalues and eigenvectors of A by following the steps above.\n",
    "$$A=\\begin{pmatrix}4 & -2\\\\5 & -7\\end{pmatrix}$$\n",
    "\n",
    "1. First set $det(A-\\lambda I)=0$:\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{pmatrix}\n",
    "4-\\lambda & -2\\\\\n",
    "5 & -7-\\lambda\n",
    "\\end{pmatrix}\n",
    "\\right|\n",
    "=0\n",
    "$$\n",
    "\n",
    "2. Take the determinant to find the characteristic equation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\chi_A(\\lambda)&=&(4-\\lambda)(-7-\\lambda)+10=0\\\\\n",
    "&=&\\lambda^2+3\\lambda-18=0\n",
    "\\end{eqnarray}\n",
    "\n",
    "3. Find the roots/eigenvalues.\n",
    "\n",
    "$$\\chi_A(\\lambda)=(\\lambda+6)(\\lambda-3)$$\n",
    "\n",
    "The order of assigning $\\lambda_1$ and $\\lambda_2$ doesn't really matter. I will use these:\n",
    "$$\\lambda_1=3$$ \n",
    "$$\\lambda_2=-6$$\n",
    "\n",
    "Neither of these are repeated, so the algebraic multiplicity of both are just 1.\n",
    "$$m_1=1$$ \n",
    "$$m_2=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with the example and find the eigenvectors of A.\n",
    "- For $\\lambda_1=3$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}\n",
    "4-\\lambda & -2\\\\\n",
    "5 & -7-\\lambda\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "4-3 & -2\\\\\n",
    "5 & -7-3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "1 & -2\\\\\n",
    "5 & -10\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Notice that both rows of A when multiplied by x give the same equation:\n",
    "\\begin{eqnarray}\n",
    "&x_1&-2x_2=0 \\\\\n",
    "\\Rightarrow &x_1&=2x_2\n",
    "\\end{eqnarray}\n",
    "Now we can solve for x:\n",
    "$$\n",
    "x=\n",
    "\\begin{pmatrix}2x_2 \\\\ x_2\\end{pmatrix}=\n",
    "\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}x_2\n",
    "$$\n",
    "An eigenvector associated with $\\lambda_1$, $\\mathbf{v_1}$, is any scalar multiple of $\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$.\n",
    "\n",
    "- For $\\lambda_2=-6$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}\n",
    "4-\\lambda & -2\\\\\n",
    "5 & -7-\\lambda\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "4+6 & -2\\\\\n",
    "5 & -7+6\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "10 & -2\\\\\n",
    "5 & -1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}&=&\\begin{pmatrix}0\\\\0\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Notice that both rows of A when multiplied by x give the same equation:\n",
    "\\begin{eqnarray}\n",
    "&5x_1&-x_2=0 \\\\\n",
    "\\Rightarrow &x_2&=5x_1\n",
    "\\end{eqnarray}\n",
    "Now we can solve for x:\n",
    "$$\n",
    "x=\n",
    "\\begin{pmatrix}x_1 \\\\ 5x_1\\end{pmatrix}=\n",
    "\\begin{pmatrix}1 \\\\ 5\\end{pmatrix}x_1\n",
    "$$\n",
    "An eigenvector associated with $\\lambda_2$, $\\mathbf{v_2}$, is any scalar multiple of $\\begin{pmatrix}1 \\\\ 5\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue decomposition\n",
    "The eigenvalue decomposition of A is as such:\n",
    "$$A=S\\Lambda S^{-1}$$\n",
    "\n",
    "Where,\n",
    "$$\n",
    "\\Lambda=diag([\\lambda_1, \\lambda_2, \\cdots, \\lambda_n])\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & \\cdots & \\lambda_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S=\\begin{pmatrix}\n",
    "\\mathbf{v_1} & \\mathbf{v_2} & \\cdots & \\mathbf{v_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "__Note:__ it does not matter in which order the eigenvalues are placed to form $\\Lambda$. However, for $A=S\\Lambda S^{-1}$ to be true, the position of the column of $\\lambda_i$ must match the position of the column of $\\mathbf{v_i}$ (where $\\mathbf{v_i}$ is an eigenvector corresponding to $\\lambda_i$).\n",
    "\n",
    "__Def:__ Geometric multiplicity, $q_i$, of $\\lambda_i$ is the number of linearly independent eigenvectors that can be formed from $\\lambda_i$. Also, $1\\leq q_i\\leq m_i$.\n",
    "This means at least 1 eigenvector can be formed from every eigenvalue, but at most $m_i$. For example: if an eigenvalue is repeated 3 times, there will be at most 3 corresponding linearly independent eigenvectors but at least 1 eigenvector.\n",
    "\n",
    "The geometric multiplicity of an eigenvalue is a very important concept for eigenvalue decomposition. In order to perform the eigenvalue decomposition, as described above where $A=S\\Lambda S^{-1}$, the geomectric multiplicity must be equal to the algebraic multiplicity for each eigenvalue. i.e.\n",
    "\n",
    "$$m_i=q_i\\ \\ \\ \\forall\\lambda_i$$\n",
    "\n",
    "This means that however many times an eigenvalue is repeated, there must be that same number of linearly independent eigenvectors derived from that eigenvalue. Otherwise, there isn't enough eigenvectors to fill the columns of S.\n",
    "\n",
    "When $m_i\\neq q_i$, there is a process to find what are called generalized eigenvectors to fill up the missing columns of S. When this happens, $\\Lambda$ is replaced with $J$ and is not strictly diagonal. This is called the Jordan form and is a topic for another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun Facts\n",
    "\n",
    "- If eigenvalues are complex, the corresponding eigenvectors will be complex. \n",
    "- If an eigenvalue is complex, its complex conjugate will also be an eigenvalue. \n",
    "- If an eigenvector is complex, its complex conjugate will also be an eigenvector.\n",
    "- Symmetric matrices have real eigenvalues.\n",
    "- For a positive definite matrix $\\Leftrightarrow$ all eigenvalues will be positive ($\\lambda_i>0$).\n",
    "- For a positive semi-definite matrix $\\Leftrightarrow$ all eigenvalues will be non-negative ($\\lambda_i\\geq0$).\n",
    "- For a negative semi-definite matrix $\\Leftrightarrow$ all eigenvalues will be non-positive ($\\lambda_i\\leq0$).\n",
    "- For a negative definite matrix $\\Leftrightarrow$ all eigenvalues will be negative ($\\lambda_i<0$).\n",
    "- The trace of a matrix equals the sum of its eigenvalues ($trace(A)=\\sum\\lambda_i$).\n",
    "- The determinant of a matrix equals the product of its eigenvalues ($det(A)=\\Pi\\lambda_i$).\n",
    "- Eigenvectors of a matrix are all linea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Numerical Examples\n",
    "\n",
    "### First let's verify the example from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "Test 1:\n",
      "v1 and l1 are a right eigen pair! \n",
      "\n",
      "Test 2:\n",
      "v2 and l2 are a right eigen pair! \n",
      "\n",
      "Test 3:\n",
      "A = S*L*S_inv! \n",
      "\n",
      "Test 4:\n",
      "It still works when the order is switched! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4,-2],\n",
    "              [5,-7]])\n",
    "l1 = 3\n",
    "l2 = -6\n",
    "v1 = np.array([[2],\n",
    "               [1]])\n",
    "v2 = np.array([[1],\n",
    "               [5]])\n",
    "\n",
    "print(np.trace(A))\n",
    "\n",
    "# check that Ax-lx=0\n",
    "# note that numerical precision might limit this to being very close to true\n",
    "print('Test 1:')\n",
    "if np.array_equal(A@v1, l1*v1):\n",
    "    print('v1 and l1 are a right eigen pair! \\n')\n",
    "else:\n",
    "    print('error \\n')\n",
    "    \n",
    "print('Test 2:')\n",
    "if np.array_equal(A@v2, l2*v2):\n",
    "    print('v2 and l2 are a right eigen pair! \\n')\n",
    "else:\n",
    "    print('error \\n')\n",
    "    \n",
    "# eigenvalue decomposition\n",
    "L = np.diag([l1, l2])\n",
    "S = np.concatenate((v1,v2), axis=1)\n",
    "S_inv = np.linalg.inv(S)\n",
    "\n",
    "# test that it works\n",
    "print('Test 3:')\n",
    "if np.array_equal(A, S@L@S_inv):\n",
    "    print('A = S*L*S_inv! \\n')\n",
    "else:\n",
    "    print('error \\n')\n",
    "\n",
    "# now let's test that the order doesn't matter\n",
    "L = np.diag([l2, l1])\n",
    "S = np.concatenate((v2,v1), axis=1)\n",
    "S_inv = np.linalg.inv(S)\n",
    "\n",
    "print('Test 4:')\n",
    "if np.array_equal(A, S@L@S_inv):\n",
    "    print('It still works when the order is switched! \\n')\n",
    "else:\n",
    "    print('error \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see how to get the eigenvalues through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues are 11, 1, and 2.\n",
      "The eigenvector corresponding to 11 is:\n",
      "[[0.        ]\n",
      " [0.4472136 ]\n",
      " [0.89442719]]\n",
      "The eigenvector corresponding to 1 is:\n",
      "[[ 0.        ]\n",
      " [ 0.89442719]\n",
      " [-0.4472136 ]]\n",
      "The eigenvector corresponding to 2 is:\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "success\n",
      "fail\n",
      "success\n",
      "A - S*Lambda*S_inv = \n",
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 4.4408921e-16 0.0000000e+00]\n",
      " [0.0000000e+00 4.4408921e-16 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[2.,0.,0.],\n",
    "              [0.,3.,4.],\n",
    "              [0.,4.,9.]])\n",
    "\n",
    "vals,vecs = np.linalg.eig(A)\n",
    "\n",
    "print('Eigenvalues are %d, %d, and %d.' % (vals[0],vals[1],vals[2]))\n",
    "print('The eigenvector corresponding to %d is:'%vals[0])\n",
    "print(vecs[:,0][:,None])\n",
    "print('The eigenvector corresponding to %d is:'%vals[1])\n",
    "print(vecs[:,1][:,None])\n",
    "print('The eigenvector corresponding to %d is:'%vals[2])\n",
    "print(vecs[:,2][:,None])\n",
    "\n",
    "# check if Ax-lx=0\n",
    "if np.array_equal(A@vecs[:,0], vals[0]*vecs[:,0]) :\n",
    "    print('success')\n",
    "else:\n",
    "    print('fail')\n",
    "if np.array_equal(A@vecs[:,1], vals[1]*vecs[:,1]) :\n",
    "    print('success')\n",
    "else:\n",
    "    print('fail')\n",
    "if np.array_equal(A@vecs[:,2], vals[2]*vecs[:,2]) :\n",
    "    print('success')\n",
    "else:\n",
    "    print('fail')\n",
    "    \n",
    "# A@vecs[:,1] - vals[1]*vecs[:,1]\n",
    "\n",
    "# Eigenvalue decomposition\n",
    "S = vecs\n",
    "Lambda = np.diag([vals[0],vals[1],vals[2]])\n",
    "S_inv = np.linalg.inv(S)\n",
    "\n",
    "# Check that A = S*Lambda*S_inv\n",
    "print('A - S*Lambda*S_inv = ')\n",
    "print(A - S@Lambda@S_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Engineering Application\n",
    "Ideas\n",
    "- Inertia tensor, find axes where there are no cross terms\n",
    "- Differential equation or difference equation (poles of the system show response)\n",
    "- Rotation matrix (eigenvalues will be 1, $e^{j\\theta}$, $e^{-j\\theta}$ and eigenvector corresponding to 1 is the axis of rotation)\n",
    "\n",
    "Provide a more sophisticated example showing one engineering example of the topic, complete with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
